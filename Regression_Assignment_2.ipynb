{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Regression Assignment_2"
      ],
      "metadata": {
        "id": "C8qtOLxmGWo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvVlUtM1GBZV"
      },
      "outputs": [],
      "source": [
        "Q1. Explain the concept of R-squared in linear regression models.\n",
        "R-squared (Coefficient of Determination):\n",
        "\n",
        "Definition: R-squared is a statistical measure that represents the proportion of the variance in the dependent variable (target) that is explained by the independent variables (features) in a regression model.\n",
        "Interpretation: R-squared ranges from 0 to 1, where 1 indicates that the model explains all the variability of the response data around its mean."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
        "Adjusted R-squared:\n",
        "\n",
        "Definition: Adjusted R-squared is a modification of R-squared that adjusts for the number of predictors (independent variables) in the model.\n",
        "Purpose: Adjusted R-squared penalizes excessive use of predictors that do not significantly contribute to explaining the variation in the dependent variable."
      ],
      "metadata": {
        "id": "fCNOTtgtHVn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. When is it more appropriate to use adjusted R-squared?\n",
        "Appropriate Use of Adjusted R-squared:\n",
        "\n",
        "Comparative Analysis: Use adjusted R-squared when comparing models with different numbers of predictors.\n",
        "Model Selection: It helps in selecting the most parsimonious model (simplest model with the best fit) that explains the data adequately without unnecessary complexity."
      ],
      "metadata": {
        "id": "oScGwDJKHVv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q4. What are RMSE, MSE, and MAE in the context of regression analysis?\n",
        "RMSE (Root Mean Squared Error): RMSE represents the square root of the average of squared differences between predicted and actual values.\n",
        "\n",
        "MSE (Mean Squared Error): MSE represents the average of squared differences between predicted and actual values.\n",
        "\n",
        "MAE (Mean Absolute Error): MAE represents the average of absolute differences between predicted and actual values.\n",
        "\n",
        "Interpretation: Lower values of RMSE, MSE, and MAE indicate better model performance in terms of prediction accuracy."
      ],
      "metadata": {
        "id": "fRjjp6gmHV0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
        "Advantages:\n",
        "\n",
        "RMSE and MSE: Penalize large errors more than MAE, making them sensitive to outliers.\n",
        "MAE: Less sensitive to outliers due to its absolute nature.\n",
        "Disadvantages:\n",
        "\n",
        "RMSE and MSE: Heavily influenced by outliers and can be biased towards models with larger errors.\n",
        "MAE: Does not differentiate between the magnitude of errors.\n",
        "Choice of Metric: Depends on the specific problem and the importance of outliers in the context of the application."
      ],
      "metadata": {
        "id": "SaB8olr7HV3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
        "Lasso Regularization:\n",
        "\n",
        "Concept: Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty term equivalent to the absolute value of the magnitude of coefficients to the loss function of the linear model.\n",
        "Purpose: Encourages sparsity by shrinking some coefficients to zero, effectively performing feature selection.\n",
        "Difference from Ridge: Ridge uses a penalty equivalent to the square of the magnitude of coefficients (L2 norm), which tends to shrink coefficients towards zero without eliminating them entirely.\n",
        "Appropriate Use:\n",
        "\n",
        "Feature Selection: When there are many predictors and some are irrelevant or redundant, Lasso can automatically select the most relevant ones by setting others to zero.\n",
        "Sparse Solutions: When the goal is to obtain a simpler and more interpretable model."
      ],
      "metadata": {
        "id": "MJH9F-2xHV6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
        "Preventing Overfitting:\n",
        "\n",
        "Regularization: Regularized linear models like Ridge and Lasso penalize large coefficients, thereby reducing model complexity.\n",
        "Example: In Ridge regression, the penalty term ensures that the model does not fit the noise in the training data, leading to better generalization on unseen data."
      ],
      "metadata": {
        "id": "gmEONUv7HV-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
        "Limitations:\n",
        "\n",
        "Model Complexity: Choosing the regularization parameter Î± can be challenging.\n",
        "Loss of Information: High regularization can lead to underfitting if important predictors are penalized too heavily.\n",
        "Assumptions: Regularization assumes a linear relationship between predictors and the response variable, which may not always hold true.\n",
        "Best Choice: Regularized linear models are effective when the relationship between predictors and response is linear and there is a need to prevent overfitting. For non-linear relationships or when interpretability is crucial, other models might be more appropriate."
      ],
      "metadata": {
        "id": "1cC003d9JNHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
        "Choice of Better Performer:\n",
        "\n",
        "Comparison: Model B with MAE of 8 is generally preferred because it indicates smaller average prediction errors compared to Model A with RMSE of 10.\n",
        "Limitations: MAE does not penalize large errors as heavily as RMSE, so Model A might be more suitable if avoiding large errors is critical."
      ],
      "metadata": {
        "id": "N5npdu-hJNNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
        "Comparison of Regularized Models:\n",
        "\n",
        "Evaluation: Choose the model with better performance metrics (e.g., lower cross-validation error).\n",
        "Trade-offs:\n",
        "Ridge (L2): Smooths coefficient estimates but does not perform variable selection.\n",
        "Lasso (L1): Performs variable selection by setting some coefficients to zero, leading to a sparse solution but can be sensitive to correlated predictors.\n",
        "Selection Criteria: Depends on the specific problem:\n",
        "\n",
        "Ridge: Better when all predictors are potentially useful and multicollinearity is a concern.\n",
        "Lasso: Better when feature selection is important or when there are many predictors with potential redundancy."
      ],
      "metadata": {
        "id": "2iVXinWcJNSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2mdXI-XsJNdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}