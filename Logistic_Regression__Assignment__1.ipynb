{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression\n"
      ],
      "metadata": {
        "id": "C8qtOLxmGWo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvVlUtM1GBZV"
      },
      "outputs": [],
      "source": [
        "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate.\n",
        "Difference:\n",
        "\n",
        "Linear Regression:\n",
        "\n",
        "Purpose: Predicts continuous numeric outcomes.\n",
        "Output: Predicted values can range from negative to positive infinity.\n",
        "Example: Predicting house prices based on features like area, number of rooms, location, etc.\n",
        "Logistic Regression:\n",
        "\n",
        "Purpose: Predicts binary outcomes (0 or 1, Yes or No).\n",
        "Output: Predicts probabilities between 0 and 1 using a logistic (sigmoid) function.\n",
        "Example: Predicting whether a customer will buy a product (Yes/No) based on demographic and behavioral features.\n",
        "Scenario where logistic regression is more appropriate:\n",
        "\n",
        "Example: Predicting the likelihood of a patient having a certain disease based on symptoms and medical history.\n",
        "Reason: The outcome variable is binary (presence or absence of disease), making logistic regression suitable for predicting probabilities of discrete outcomes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n",
        "Regularization:\n",
        "\n",
        "Purpose: Controls overfitting by adding a penalty term to the cost function that discourages learning overly complex models.\n",
        "Types:\n",
        "L1 Regularization (Lasso):\n",
        "L2 Regularization (Ridge):\n",
        "Impact: Increases bias slightly but reduces variance, improving generalization to new data."
      ],
      "metadata": {
        "id": "oScGwDJKHVv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?\n",
        "ROC Curve (Receiver Operating Characteristic Curve):\n",
        "\n",
        "Definition: Graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied.\n",
        "X-axis: False Positive Rate (FPR)\n",
        "Y-axis: True Positive Rate (TPR)\n",
        "Purpose: Evaluates the trade-off between sensitivity (TPR) and specificity (1 - FPR).\n",
        "Area Under Curve (AUC): AUC near 1 indicates a good classifier; AUC near 0.5 indicates a poor classifier."
      ],
      "metadata": {
        "id": "fRjjp6gmHV0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?\n",
        "Feature Selection Techniques:\n",
        "\n",
        "Correlation Matrix: Remove highly correlated variables.\n",
        "Forward Selection: Add one feature at a time, starting with the most significant.\n",
        "Backward Elimination: Remove one feature at a time, starting with the least significant.\n",
        "L1 Regularization (Lasso): Encourages sparsity in the coefficient vector, effectively performing feature selection by shrinking less important features' coefficients to zero.\n",
        "Improvement in Performance:\n",
        "\n",
        "Reduces overfitting by selecting the most relevant features.\n",
        "Improves model interpretability and reduces computation time."
      ],
      "metadata": {
        "id": "SaB8olr7HV3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?\n",
        "Handling Imbalanced Datasets:\n",
        "\n",
        "Strategies:\n",
        "Resampling:\n",
        "Oversampling: Increase minority class examples (SMOTE, ADASYN).\n",
        "Undersampling: Decrease majority class examples.\n",
        "Cost-Sensitive Learning: Assign higher misclassification costs to minority class.\n",
        "Ensemble Methods: Use algorithms that handle class imbalance (e.g., Balanced Random Forest, EasyEnsemble).\n",
        "Synthetic Data Generation: Generate synthetic data points for the minority class.\n",
        "Importance:\n",
        "\n",
        "Ensures the model doesn’t become biased towards the majority class, leading to poor predictive performance on the minority class."
      ],
      "metadata": {
        "id": "MJH9F-2xHV6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?\n",
        "Common Issues:\n",
        "\n",
        "Multicollinearity: High correlation among independent variables can lead to unstable estimates of coefficients.\n",
        "\n",
        "Solution: Use regularization techniques (e.g., Ridge regression) to shrink coefficients, reducing the impact of multicollinearity.\n",
        "Alternative: Remove one of the highly correlated variables or use dimensionality reduction techniques like PCA.\n",
        "Non-linearity: Logistic regression assumes a linear relationship between independent variables and the log-odds of the dependent variable.\n",
        "\n",
        "Solution: Transform variables or use polynomial logistic regression to capture non-linear relationships.\n",
        "Outliers: Outliers can disproportionately influence model parameters.\n",
        "\n",
        "Solution: Remove outliers or use robust techniques that are less sensitive to outliers.\n",
        "Model Interpretability: Logistic regression coefficients are interpretable but assume linear relationships.\n",
        "\n",
        "Solution: Regularize the model to simplify interpretation or use more complex models if linear assumptions don’t hold."
      ],
      "metadata": {
        "id": "gmEONUv7HV-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1cC003d9JNHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N5npdu-hJNNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2iVXinWcJNSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2mdXI-XsJNdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}