{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Regression Assignment_1"
      ],
      "metadata": {
        "id": "C8qtOLxmGWo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvVlUtM1GBZV"
      },
      "outputs": [],
      "source": [
        "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
        "Simple Linear Regression:\n",
        "\n",
        "Definition: Simple linear regression involves predicting a response variable y based on a single predictor variable x.\n",
        "Example: Predicting house prices based on the area of the house (single predictor).\n",
        "\n",
        "Multiple Linear Regression:\n",
        "\n",
        "Definition: Multiple linear regression involves predicting a response variable y based on multiple predictor variables x1,x2,x3...\n",
        "Example: Predicting house prices based on area, number of bedrooms, and location (multiple predictors)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
        "Assumptions of Linear Regression:\n",
        "\n",
        "Linearity: The relationship between the predictors and the response variable should be linear.\n",
        "Independence of errors: The errors (residuals) should be independent of each other.\n",
        "Homoscedasticity (Constant Variance): The variance of the errors should be constant across all levels of the predictor variables.\n",
        "Normality of errors: The errors should be normally distributed.\n",
        "No perfect multicollinearity: The predictors should not be perfectly correlated with each other.\n",
        "Checking Assumptions:\n",
        "\n",
        "Residual plots: Check for linearity, constant variance, and normality of residuals.\n",
        "Durbin-Watson test: Checks for independence of residuals.\n",
        "Jarque-Bera test: Tests for normality of residuals.\n",
        "Variance inflation factor (VIF): Checks for multicollinearity among predictors."
      ],
      "metadata": {
        "id": "fCNOTtgtHVn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
        "Gradient Descent:\n",
        "\n",
        "Concept: Gradient descent is an optimization algorithm used to minimize the cost function (error) in machine learning models.\n",
        "Process: It iteratively adjusts model parameters (weights) in the direction of the steepest descent of the cost function gradient until convergence is achieved.\n",
        "Importance: Used in training models like linear regression, neural networks, and other optimization problems."
      ],
      "metadata": {
        "id": "oScGwDJKHVv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
        "Multiple Linear Regression:\n",
        "\n",
        "Definition: Multiple linear regression involves predicting a response variable y using multiple predictor variables x1,x2...\n",
        "Difference from Simple Linear Regression: Multiple linear regression uses more than one predictor variable, whereas simple linear regression uses only one predictor variable."
      ],
      "metadata": {
        "id": "fRjjp6gmHV0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
        "Multicollinearity:\n",
        "\n",
        "Concept: Multicollinearity occurs when predictor variables in a multiple regression model are highly correlated with each other.\n",
        "Issues: It can inflate standard errors, make coefficients unstable, and make it difficult to assess the individual contribution of predictors.\n",
        "Detection: Use methods like correlation matrix, variance inflation factor (VIF), or eigenvalues of the correlation matrix.\n",
        "Addressing: Remove highly correlated predictors, combine them into a single predictor, or use regularization techniques like Ridge regression."
      ],
      "metadata": {
        "id": "SaB8olr7HV3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. Describe the polynomial regression model. How is it different from linear regression?\n",
        "Polynomial Regression:\n",
        "\n",
        "Definition: Polynomial regression is a form of regression analysis where the relationship between the predictor variables and the response variable is modeled as an nth degree polynomial.\n",
        "Difference from Linear Regression: Linear regression models a linear relationship between variables (straight line), whereas polynomial regression models a non-linear relationship (curve)."
      ],
      "metadata": {
        "id": "MJH9F-2xHV6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
        "Advantages:\n",
        "\n",
        "Captures non-linear relationships: Useful when the relationship between predictors and response is not linear.\n",
        "Flexible: Can fit a wide range of curves depending on the degree of the polynomial.\n",
        "Disadvantages:\n",
        "\n",
        "Overfitting: Higher degree polynomials can overfit the training data.\n",
        "Interpretability: Interpretation of results becomes more complex with higher degree polynomials.\n",
        "Use Cases:\n",
        "\n",
        "Advantageous: When data exhibit a non-linear trend or relationship.\n",
        "Preferable: When simpler models like linear regression fail to capture the complexity of the data."
      ],
      "metadata": {
        "id": "gmEONUv7HV-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}